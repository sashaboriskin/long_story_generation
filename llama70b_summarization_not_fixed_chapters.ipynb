{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcd3fc00-9860-4f23-8ca9-27c427f14e05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T16:34:17.163764Z",
     "iopub.status.busy": "2024-07-15T16:34:17.163431Z",
     "iopub.status.idle": "2024-07-15T16:34:22.260493Z",
     "shell.execute_reply": "2024-07-15T16:34:22.259961Z",
     "shell.execute_reply.started": "2024-07-15T16:34:17.163747Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install vllm -q\n",
    "from vllm import SamplingParams, LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b21cc227-61ff-411c-a7aa-578c278ab7a9",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-07-15T16:34:22.261588Z",
     "iopub.status.busy": "2024-07-15T16:34:22.261434Z",
     "iopub.status.idle": "2024-07-15T16:44:54.791915Z",
     "shell.execute_reply": "2024-07-15T16:44:54.791417Z",
     "shell.execute_reply.started": "2024-07-15T16:34:22.261571Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-15 16:34:22 config.py:698] Defaulting to use mp for distributed inference\n",
      "INFO 07-15 16:34:22 llm_engine.py:169] Initializing an LLM engine (v0.5.1) with config: model='meta-llama/Meta-Llama-3-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=meta-llama/Meta-Llama-3-70B-Instruct, use_v2_block_manager=False, enable_prefix_caching=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=126613)\u001b[0;0m INFO 07-15 16:34:23 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=126614)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=126615)\u001b[0;0m INFO 07-15 16:34:23 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 07-15 16:34:23 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 07-15 16:34:24 utils.py:741] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=126615)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=126614)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=126613)\u001b[0;0m INFO 07-15 16:34:24 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 07-15 16:34:24 utils.py:741] Found nccl from library libnccl.so.2\n",
      "INFO 07-15 16:34:24 utils.py:741] Found nccl from library libnccl.so.2\n",
      "INFO 07-15 16:34:24 utils.py:741] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=126615)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=126614)\u001b[0;0m INFO 07-15 16:34:24 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 07-15 16:34:24 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=126613)\u001b[0;0m INFO 07-15 16:34:24 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 07-15 16:34:26 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /home/jovyan/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=126613)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=126614)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=126615)\u001b[0;0m INFO 07-15 16:34:26 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /home/jovyan/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 07-15 16:34:26 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /home/jovyan/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 07-15 16:34:26 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /home/jovyan/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 07-15 16:34:27 weight_utils.py:218] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=126614)\u001b[0;0m INFO 07-15 16:34:27 weight_utils.py:218] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=126613)\u001b[0;0m INFO 07-15 16:34:27 weight_utils.py:218] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=126615)\u001b[0;0m INFO 07-15 16:34:27 weight_utils.py:218] Using model weights format ['*.safetensors']\n",
      "INFO 07-15 16:44:27 model_runner.py:255] Loading model weights took 32.8599 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=126615)\u001b[0;0m INFO 07-15 16:44:27 model_runner.py:255] Loading model weights took 32.8599 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=126613)\u001b[0;0m INFO 07-15 16:44:27 model_runner.py:255] Loading model weights took 32.8599 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=126614)\u001b[0;0m INFO 07-15 16:44:27 model_runner.py:255] Loading model weights took 32.8599 GB\n",
      "INFO 07-15 16:44:30 distributed_gpu_executor.py:56] # GPU blocks: 25265, # CPU blocks: 3276\n",
      "INFO 07-15 16:44:44 model_runner.py:924] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 07-15 16:44:44 model_runner.py:928] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=126614)\u001b[0;0m INFO 07-15 16:44:44 model_runner.py:924] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=126614)\u001b[0;0m INFO 07-15 16:44:44 model_runner.py:928] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=126613)\u001b[0;0m INFO 07-15 16:44:45 model_runner.py:924] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=126613)\u001b[0;0m INFO 07-15 16:44:45 model_runner.py:928] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=126615)\u001b[0;0m INFO 07-15 16:44:45 model_runner.py:924] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=126615)\u001b[0;0m INFO 07-15 16:44:45 model_runner.py:928] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 07-15 16:44:54 custom_all_reduce.py:219] Registering 5635 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=126613)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=126615)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=126614)\u001b[0;0m INFO 07-15 16:44:54 custom_all_reduce.py:219] Registering 5635 cuda graph addresses\n",
      "INFO 07-15 16:44:54 custom_all_reduce.py:219] Registering 5635 cuda graph addresses\n",
      "INFO 07-15 16:44:54 custom_all_reduce.py:219] Registering 5635 cuda graph addresses\n",
      "INFO 07-15 16:44:54 model_runner.py:1117] Graph capturing finished in 10 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=126613)\u001b[0;0m INFO 07-15 16:44:54 model_runner.py:1117] Graph capturing finished in 10 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=126615)\u001b[0;0m INFO 07-15 16:44:54 model_runner.py:1117] Graph capturing finished in 10 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=126614)\u001b[0;0m INFO 07-15 16:44:54 model_runner.py:1117] Graph capturing finished in 10 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"meta-llama/Meta-Llama-3-70B-Instruct\", \n",
    "          tensor_parallel_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbe70e0-3bda-4737-8b57-76bcc6d0c5d8",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-15T16:30:58.382652Z",
     "iopub.status.idle": "2024-07-15T16:30:58.382845Z",
     "shell.execute_reply": "2024-07-15T16:30:58.382762Z",
     "shell.execute_reply.started": "2024-07-15T16:30:58.382752Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install transformers -q\n",
    "import re\n",
    "from transformers import ( \n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM\n",
    ")\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce765fc2-d6aa-44a1-a957-999980afcda0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T16:55:17.971769Z",
     "iopub.status.busy": "2024-07-15T16:55:17.971228Z",
     "iopub.status.idle": "2024-07-15T16:55:17.975843Z",
     "shell.execute_reply": "2024-07-15T16:55:17.975250Z",
     "shell.execute_reply.started": "2024-07-15T16:55:17.971748Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summarization(text):\n",
    "    WHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n",
    "    \n",
    "    input_ids = summary_tokenizer(\n",
    "        [WHITESPACE_HANDLER(text)],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    output_ids = summary_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=1000,\n",
    "        no_repeat_ngram_size=2,\n",
    "        num_beams=4\n",
    "    )[0]\n",
    "\n",
    "    summary = summary_tokenizer.decode(\n",
    "        output_ids,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67effaba-852c-45e0-821e-6d634ac7ef44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T16:55:20.818222Z",
     "iopub.status.busy": "2024-07-15T16:55:20.817781Z",
     "iopub.status.idle": "2024-07-15T16:55:20.820798Z",
     "shell.execute_reply": "2024-07-15T16:55:20.820358Z",
     "shell.execute_reply.started": "2024-07-15T16:55:20.818204Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_words(s):\n",
    "    return len(s.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dfb66b3-5190-40ab-abac-622776371eda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T16:55:24.686157Z",
     "iopub.status.busy": "2024-07-15T16:55:24.685591Z",
     "iopub.status.idle": "2024-07-15T16:55:31.997391Z",
     "shell.execute_reply": "2024-07-15T16:55:31.996673Z",
     "shell.execute_reply.started": "2024-07-15T16:55:24.686136Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:562: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3-70B-Instruct')\n",
    "summary_model_name = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
    "summary_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "summary_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e755703-6e6f-4b8f-a471-51bada547fde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T16:56:01.619869Z",
     "iopub.status.busy": "2024-07-15T16:56:01.619344Z",
     "iopub.status.idle": "2024-07-15T16:56:01.623174Z",
     "shell.execute_reply": "2024-07-15T16:56:01.622729Z",
     "shell.execute_reply.started": "2024-07-15T16:56:01.619850Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start of the story\n",
    "test_prompt = \"\"\"\n",
    "Theodore Nott had been eagerly awaiting his invitation to attend Hogwarts School of Witchcraft and Wizardry from the very beginning. He knew he was special, believed in it. He had always taken pride in his family. Theo grew up as an unassuming, calm, and intelligent child, but with a keen desire to achieve the best, clever enough. And, as the son of the Notts, how could he not end up in Slytherin? When he turned 11, he received the coveted letter to the wizarding school, and in September, after the sorting ceremony, he found himself in the Snake House. He liked everything about it: the aristocratic common room, most of his classmates (except Crabbe and Goyle), the head of the house, and the subjects. Theo developed a great fondness for Potions and Transfiguration, believing that they were essential for life and the future. Any subject could be transformed into what you desired. Pansy Parkinson also hailed from a pure-blood wizarding family. From childhood, she was told to look perfect, know how to flirt with men, and, above all, seek wealth. Pansy was very intelligent herself, but despite her beauty, she had apparently heard many different nicknames in kindergarten and elementary school. The most offensive was \"Pug.\" Well, she didn't look like one; it was just a detail kids used for insults. But in Slytherin at Hogwarts, you wouldn't hear such things. Pansy understood from the first days that Draco Malfoy was obviously the best option for her, both as a friend and in hypothetical relationships. In the future, of course. She loved Ancient Runes and Divination; she enjoyed dueling and exploring aspects of the future. And runes were always interesting to lay out: you never knew what answer they would give, whether it would be Gebo with Wunjo or something else. The years passed, Theo and Pansy grew up. Now it was the fifth year, they were 15, the first exam... Now, instead of \"Pug,\" the girl heard the occasional very quiet \"slut,\" and Theo suddenly began to hear \"womanizer,\" although he just preferred studying, didn't invest much in relationships, because grades for the O.W.L.s were much more important, so it never came to intimacy. So, initially, they judge based on appearance, and then they see an almost emotionless guy. And Pansy just got used to Draco, got used to the fact that he didn't care about her, just occasionally drank, but never allowed herself to kiss someone else or, worse, intimacy. Neither paid much attention to rumors, just lived and continued to prepare. And finally, all O.W.L.s were passed with grades above expectations, and the sixth year began. Theo and Pansy didn't interact much before, but they got along quite well. There was never any misunderstanding between them. And both occasionally noticed each other's successes and good qualities. One day, they both sat on the lawn in front of the castle. There was still an hour before classes; Transfiguration was supposed to be next. Theo had mastered non-verbal spells, so his cup easily turned into either a tangerine or a plate. • How do you manage that? - suddenly asked Parkinson. • She had problems with this Transfiguration spell. So, she definitely needed help, and Theo usually didn't refuse. • It's quite simple if you repeat not only the words and movements but also know what you want to get from the subject in the end. What it should be like, what smell, taste, color, textures... • Everything you say sounds so simple, - the girl sighed, - but I just can't... • Want me to prove that it's really easy? And also, - here Theo smiled, - a bright and beautiful girl, resembling a star, certainly won't mind having a reason to rejoice. Flowers for the lady? • The girl snorted. Clearly, he was flirting, but at the same time, it was beautiful, and he was teaching. • Okay, but the flowers should be worthy of me. • Of course, princess, - the guy looks for the nearest tree branch, finds it, takes it, frowns slightly, while imagining beautiful black orchids that would suit Pansy, - Orchideus, - Theo said, after which the wand from the branch turned into exactly the bouquet Nott wanted. He handed her the bouquet with a smile.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66a7217c-9448-4c90-8058-b00fdbbfbda8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T16:56:03.532314Z",
     "iopub.status.busy": "2024-07-15T16:56:03.531914Z",
     "iopub.status.idle": "2024-07-15T16:56:03.535332Z",
     "shell.execute_reply": "2024-07-15T16:56:03.534872Z",
     "shell.execute_reply.started": "2024-07-15T16:56:03.532294Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_config = SamplingParams(\n",
    "    temperature=0.5,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.2, \n",
    "    top_k=60, \n",
    "    max_tokens=4000,\n",
    ")\n",
    "\n",
    "chapter_config = SamplingParams(\n",
    "    temperature=0.5,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.3, \n",
    "    top_k=80, \n",
    "    max_tokens=4000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb1cb3b1-5f6b-4bbd-a8e8-ca4002621430",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T16:56:08.957635Z",
     "iopub.status.busy": "2024-07-15T16:56:08.957047Z",
     "iopub.status.idle": "2024-07-15T16:56:08.960327Z",
     "shell.execute_reply": "2024-07-15T16:56:08.959897Z",
     "shell.execute_reply.started": "2024-07-15T16:56:08.957615Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prompt for generating plan\n",
    "plan_prompt = f\"\"\"\n",
    "You are a popular fanfiction creator tasked with writing an extended piece for a Harry Potter fanfiction.\n",
    "You have to come up with an interesting plot. \n",
    "Follow the classical composition and include beginning, climax and outcome in this book. Write a detailed plan for each composition chapter. In the format of \n",
    "Chapter 1: What happens in this chapter\n",
    "Chapter 2: What happens in this chapter\n",
    "...\n",
    "It is important to display the following aspects in the plot:\n",
    "Action and Conflict: Introduce conflicts and challenges that the characters must face. Whether it's a battle with dark forces, a personal dilemma, or a complex mystery, ensure there is plenty of action and tension to keep readers hooked.\n",
    "Magical Elements: Highlight the magical aspects of the Harry Potter universe. Describe new spells, potions, magical creatures, and enchanted locations. Make magic an integral part of the plot and the characters' lives.\n",
    "World-Building: Expand on the existing lore of the Harry Potter universe. Introduce new locations, traditions, and histories. Make the world feel alive and full of possibilities.\n",
    "Do not use p.s., p.p.s. and exc.\n",
    "The following text is the beginning of the first chapter of this book. Generate the summary according to this text. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa6650fa-1493-4354-b8e9-ce15c175c35a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T16:56:32.530556Z",
     "iopub.status.busy": "2024-07-15T16:56:32.530140Z",
     "iopub.status.idle": "2024-07-15T16:56:41.821924Z",
     "shell.execute_reply": "2024-07-15T16:56:41.821254Z",
     "shell.execute_reply.started": "2024-07-15T16:56:32.530535Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.25s/it, est. speed input: 129.26 toks/s, output: 50.36 toks/s]\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "        {\"role\": \"system\", \"content\": plan_prompt},\n",
    "        {\"role\": \"user\", \"content\": test_prompt}\n",
    "    ]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "summary_heading = \"\"\"\n",
    "You are a popular fanfiction creator tasked with writing an new chapter for a Harry Potter fanfiction accrding to the summary below.\n",
    "Your text should be distinct yet cohesive, maintaining the original tone and style of the Harry Potter series.\n",
    "Instructions:\n",
    "1. The text should be for secondary school students.\n",
    "2. Always narrate in the third person.\n",
    "3. Ensure that text is rich in detail and narrative depth.\n",
    "4. Avoid including any text outside of the story (e.g., meta comments, thank you notes, or personal addresses).\n",
    "5. Write the text with no additional comments.\n",
    "6. Use only English letters and Arabic numerals.\n",
    "Here is summary of whole text: \n",
    "\"\"\"\n",
    "book_summary = llm.generate(input_ids, summary_config)\n",
    "book_summary = book_summary[0].outputs[0].text\n",
    "book_summary = summary_heading + book_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cd4e3bc-7927-49bc-ab20-4574e631b1a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T16:56:41.823821Z",
     "iopub.status.busy": "2024-07-15T16:56:41.823184Z",
     "iopub.status.idle": "2024-07-15T16:56:41.827002Z",
     "shell.execute_reply": "2024-07-15T16:56:41.826572Z",
     "shell.execute_reply.started": "2024-07-15T16:56:41.823804Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a popular fanfiction creator tasked with writing an new chapter for a Harry Potter fanfiction accrding to the summary below.\n",
      "Your text should be distinct yet cohesive, maintaining the original tone and style of the Harry Potter series.\n",
      "Instructions:\n",
      "1. The text should be for secondary school students.\n",
      "2. Always narrate in the third person.\n",
      "3. Ensure that text is rich in detail and narrative depth.\n",
      "4. Avoid including any text outside of the story (e.g., meta comments, thank you notes, or personal addresses).\n",
      "5. Write the text with no additional comments.\n",
      "6. Use only English letters and Arabic numerals.\n",
      "Here is summary of whole text: \n",
      "Here is a summary of the story:\n",
      "\n",
      "**Title:** \"Beneath the Surface\"\n",
      "\n",
      "**Composition Plan:**\n",
      "\n",
      "**Chapters 1-3: Introduction and Setup**\n",
      "\n",
      "* Chapter 1: Introduce Theodore Nott and Pansy Parkinson, two students in their fifth year at Hogwarts, highlighting their backgrounds, personalities, and academic interests.\n",
      "* Chapter 2: Show the daily lives of Theo and Pansy, including their interactions with friends and acquaintances, and hint at rumors surrounding them.\n",
      "* Chapter 3: Reveal the results of their OWL exams and introduce the start of their sixth year.\n",
      "\n",
      "**Chapters 4-6: Inciting Incident and Rising Action**\n",
      "\n",
      "* Chapter 4: Theo helps Pansy with a difficult Transfiguration spell, showcasing his skills and sparking a connection between them.\n",
      "* Chapter 5: As they spend more time together, Theo and Pansy begin to notice changes within themselves, questioning their initial impressions and judgments.\n",
      "* Chapter 6: A mysterious event occurs during a Hogsmeade weekend, putting Theo and Pansy in a situation where they need to work together to uncover the truth.\n",
      "\n",
      "**Chapters 7-10: Climax and Complications**\n",
      "\n",
      "* Chapter 7: Theo and Pansy stumble upon a hidden ancient artifact, leading them down a path of discovery and danger.\n",
      "* Chapter 8: They encounter obstacles, rivalries, and secrets within Hogwarts, testing their trust and loyalty towards each other.\n",
      "* Chapter 9: A shocking revelation about their families' pasts threatens to tear them apart.\n",
      "* Chapter 10: A confrontation with a powerful foe puts Theo and Pansy's skills, friendship, and values to the ultimate test.\n",
      "\n",
      "**Chapters 11-13: Resolution and Conclusion**\n",
      "\n",
      "* Chapter 11: The aftermath of the confrontation reveals surprising alliances and unexpected consequences.\n",
      "* Chapter 12: Theo and Pansy reflect on their journey, acknowledging growth, forgiveness, and newfound understanding.\n",
      "* Chapter 13: The story concludes with a glimpse into their futures, solidifying the bond formed beneath the surface.\n",
      "\n",
      "This composition plan sets the stage for an engaging narrative filled with action, conflict, magical elements, and character development, while expanding on the rich world-building of the Harry Potter universe.\n"
     ]
    }
   ],
   "source": [
    "print(book_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98e8c156-51c8-402d-bd10-cdca239f47e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T17:02:17.379626Z",
     "iopub.status.busy": "2024-07-15T17:02:17.379062Z",
     "iopub.status.idle": "2024-07-15T17:02:17.383112Z",
     "shell.execute_reply": "2024-07-15T17:02:17.382674Z",
     "shell.execute_reply.started": "2024-07-15T17:02:17.379607Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of chapters is: 13\n"
     ]
    }
   ],
   "source": [
    "pattern = r\"(?m)^.*?\\b(\\d+-\\d+)\\b.*?$\"\n",
    "\n",
    "chapter_ranges = re.findall(pattern, book_summary)\n",
    "total_chapters = 0\n",
    "\n",
    "for range_str in chapter_ranges:\n",
    "    start_chapter, end_chapter = map(int, range_str.split('-'))\n",
    "    total_chapters += end_chapter - start_chapter + 1\n",
    "\n",
    "print(f\"The total number of chapters is: {total_chapters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6f92013-bac3-4e5f-985e-fcc569c1262d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T17:02:19.170742Z",
     "iopub.status.busy": "2024-07-15T17:02:19.170102Z",
     "iopub.status.idle": "2024-07-15T17:02:19.173398Z",
     "shell.execute_reply": "2024-07-15T17:02:19.172963Z",
     "shell.execute_reply.started": "2024-07-15T17:02:19.170724Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_of_chapter_prompt = \"\"\"\n",
    "Start generating the chapter {chapter_n} based on what happened before. \n",
    "here's what happened in the whole book so far:\n",
    "{book_summary}\n",
    "here's what happened in the previous chapter so far:\n",
    "{previous_chapter}\n",
    "Start writing the text with no additional comments.\n",
    "The structure of the begging is: \n",
    "Chapter {chapter_n}. Name of the chapter.\n",
    "Text of the chapter\n",
    "\"\"\"\n",
    "continue_chapter_prompt = \"\"\"\n",
    "Сontinue generating the chapter {chapter_n} based on what happened before.\n",
    "here's what happened in the whole book so far:\n",
    "{book_summary}\n",
    "here's what happened in the chapter so far:\n",
    "{full_chapter_context}\n",
    "Start writing the text with no additional comments.\n",
    "Do not write chapter and the name of the chapter. Just continue writing the story.\n",
    "\"\"\"\n",
    "final_of_chapter_prompt = \"\"\"\n",
    "Finish generating the chapter {chapter_n} based on what happened before. \n",
    "here's what happened in the whole book so far:\n",
    "{book_summary}\n",
    "here's what happened in the chapter so far:\n",
    "{full_chapter_context}\n",
    "Start writing the text with no additional comments.\n",
    "Do not write chapter and the name of the chapter. Just finish writing the story.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff623a45-a962-4345-9783-28456fd5e65d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T17:02:23.443188Z",
     "iopub.status.busy": "2024-07-15T17:02:23.442600Z",
     "iopub.status.idle": "2024-07-15T17:48:57.244508Z",
     "shell.execute_reply": "2024-07-15T17:48:57.243764Z",
     "shell.execute_reply.started": "2024-07-15T17:02:23.443159Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.84s/it, est. speed input: 185.61 toks/s, output: 50.22 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.81s/it, est. speed input: 169.08 toks/s, output: 50.38 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.23s/it, est. speed input: 145.89 toks/s, output: 50.55 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.82s/it, est. speed input: 204.55 toks/s, output: 48.90 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.49s/it, est. speed input: 293.10 toks/s, output: 50.09 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.45s/it, est. speed input: 390.58 toks/s, output: 49.38 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.85s/it, est. speed input: 276.67 toks/s, output: 49.78 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.51s/it, est. speed input: 336.07 toks/s, output: 48.39 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.54s/it, est. speed input: 304.24 toks/s, output: 49.58 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.80s/it, est. speed input: 435.97 toks/s, output: 48.92 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.32s/it, est. speed input: 505.38 toks/s, output: 48.75 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.11s/it, est. speed input: 177.15 toks/s, output: 50.37 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.51s/it, est. speed input: 189.98 toks/s, output: 50.32 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.31s/it, est. speed input: 245.89 toks/s, output: 50.07 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.80s/it, est. speed input: 227.59 toks/s, output: 50.00 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.63s/it, est. speed input: 311.23 toks/s, output: 49.85 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.86s/it, est. speed input: 392.38 toks/s, output: 49.44 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.13s/it, est. speed input: 386.38 toks/s, output: 49.37 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.24s/it, est. speed input: 477.70 toks/s, output: 48.72 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.60s/it, est. speed input: 506.54 toks/s, output: 48.88 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.74s/it, est. speed input: 453.87 toks/s, output: 49.08 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.65s/it, est. speed input: 432.66 toks/s, output: 48.93 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.31s/it, est. speed input: 217.84 toks/s, output: 50.22 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.28s/it, est. speed input: 204.15 toks/s, output: 50.37 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.01s/it, est. speed input: 262.98 toks/s, output: 49.97 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.82s/it, est. speed input: 284.02 toks/s, output: 49.91 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.74s/it, est. speed input: 250.54 toks/s, output: 50.07 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.92s/it, est. speed input: 355.71 toks/s, output: 49.59 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.99s/it, est. speed input: 365.92 toks/s, output: 49.61 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.06s/it, est. speed input: 412.64 toks/s, output: 49.36 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.74s/it, est. speed input: 434.93 toks/s, output: 49.22 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.18s/it, est. speed input: 558.73 toks/s, output: 48.62 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.59s/it, est. speed input: 158.38 toks/s, output: 50.33 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.15s/it, est. speed input: 194.28 toks/s, output: 50.19 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.66s/it, est. speed input: 259.79 toks/s, output: 49.97 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.68s/it, est. speed input: 222.31 toks/s, output: 50.21 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.14s/it, est. speed input: 286.88 toks/s, output: 49.82 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.66s/it, est. speed input: 269.56 toks/s, output: 49.77 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.98s/it, est. speed input: 450.16 toks/s, output: 49.13 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.79s/it, est. speed input: 324.86 toks/s, output: 49.52 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.88s/it, est. speed input: 581.52 toks/s, output: 48.44 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.05s/it, est. speed input: 506.48 toks/s, output: 48.70 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.91s/it, est. speed input: 139.76 toks/s, output: 50.40 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.51s/it, est. speed input: 161.08 toks/s, output: 50.27 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.62s/it, est. speed input: 209.05 toks/s, output: 50.10 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.37s/it, est. speed input: 293.57 toks/s, output: 49.83 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.64s/it, est. speed input: 302.44 toks/s, output: 49.73 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.68s/it, est. speed input: 487.83 toks/s, output: 48.98 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.81s/it, est. speed input: 298.32 toks/s, output: 49.82 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.09s/it, est. speed input: 433.32 toks/s, output: 49.04 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.84s/it, est. speed input: 493.59 toks/s, output: 48.91 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.32s/it, est. speed input: 477.39 toks/s, output: 48.94 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.33s/it, est. speed input: 178.81 toks/s, output: 50.50 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.30s/it, est. speed input: 202.31 toks/s, output: 50.37 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.71s/it, est. speed input: 240.46 toks/s, output: 50.04 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.99s/it, est. speed input: 194.70 toks/s, output: 50.27 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.55s/it, est. speed input: 253.35 toks/s, output: 49.95 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.27s/it, est. speed input: 370.41 toks/s, output: 49.45 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.09s/it, est. speed input: 474.17 toks/s, output: 48.95 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.26s/it, est. speed input: 422.27 toks/s, output: 49.12 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.30s/it, est. speed input: 469.24 toks/s, output: 48.92 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.44s/it, est. speed input: 471.18 toks/s, output: 48.89 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.24s/it, est. speed input: 159.53 toks/s, output: 50.35 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.80s/it, est. speed input: 160.07 toks/s, output: 50.46 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.46s/it, est. speed input: 240.12 toks/s, output: 50.09 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.47s/it, est. speed input: 239.33 toks/s, output: 50.05 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.10s/it, est. speed input: 274.15 toks/s, output: 49.91 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.54s/it, est. speed input: 459.09 toks/s, output: 49.29 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.32s/it, est. speed input: 352.23 toks/s, output: 49.51 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.02s/it, est. speed input: 353.13 toks/s, output: 49.52 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.42s/it, est. speed input: 493.80 toks/s, output: 48.85 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.75s/it, est. speed input: 528.09 toks/s, output: 48.61 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.09s/it, est. speed input: 140.66 toks/s, output: 50.52 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.16s/it, est. speed input: 170.73 toks/s, output: 50.48 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.09s/it, est. speed input: 202.45 toks/s, output: 50.30 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.83s/it, est. speed input: 236.78 toks/s, output: 50.03 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.52s/it, est. speed input: 322.40 toks/s, output: 49.71 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.32s/it, est. speed input: 345.67 toks/s, output: 49.65 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.49s/it, est. speed input: 308.77 toks/s, output: 49.70 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.91s/it, est. speed input: 306.85 toks/s, output: 49.60 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.36s/it, est. speed input: 420.22 toks/s, output: 49.11 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.44s/it, est. speed input: 149.44 toks/s, output: 50.52 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.25s/it, est. speed input: 260.35 toks/s, output: 49.97 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.51s/it, est. speed input: 236.28 toks/s, output: 50.05 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.75s/it, est. speed input: 279.26 toks/s, output: 49.95 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.75s/it, est. speed input: 180.75 toks/s, output: 50.19 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.12s/it, est. speed input: 405.00 toks/s, output: 49.41 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.62s/it, est. speed input: 314.42 toks/s, output: 49.67 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.88s/it, est. speed input: 383.14 toks/s, output: 49.34 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.35s/it, est. speed input: 642.72 toks/s, output: 48.25 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.89s/it, est. speed input: 130.25 toks/s, output: 50.62 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.55s/it, est. speed input: 188.06 toks/s, output: 50.32 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.13s/it, est. speed input: 200.89 toks/s, output: 50.25 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.38s/it, est. speed input: 244.96 toks/s, output: 50.01 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.06s/it, est. speed input: 304.11 toks/s, output: 49.81 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.44s/it, est. speed input: 340.99 toks/s, output: 49.64 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.73s/it, est. speed input: 320.16 toks/s, output: 49.73 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.08s/it, est. speed input: 424.74 toks/s, output: 49.28 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.03s/it, est. speed input: 436.36 toks/s, output: 49.05 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.46s/it, est. speed input: 404.03 toks/s, output: 49.10 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.46s/it, est. speed input: 135.43 toks/s, output: 50.46 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.79s/it, est. speed input: 180.01 toks/s, output: 50.31 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.21s/it, est. speed input: 221.64 toks/s, output: 50.06 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.92s/it, est. speed input: 255.32 toks/s, output: 49.99 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.83s/it, est. speed input: 307.52 toks/s, output: 49.77 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.06s/it, est. speed input: 382.03 toks/s, output: 49.45 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.39s/it, est. speed input: 508.29 toks/s, output: 48.79 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.28s/it, est. speed input: 508.28 toks/s, output: 48.92 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.38s/it, est. speed input: 552.13 toks/s, output: 48.74 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.21s/it, est. speed input: 118.86 toks/s, output: 50.46 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.49s/it, est. speed input: 164.67 toks/s, output: 50.34 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.20s/it, est. speed input: 245.13 toks/s, output: 50.13 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.28s/it, est. speed input: 240.53 toks/s, output: 50.00 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.88s/it, est. speed input: 275.89 toks/s, output: 49.86 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.48s/it, est. speed input: 286.85 toks/s, output: 49.77 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.42s/it, est. speed input: 397.35 toks/s, output: 49.22 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.41s/it, est. speed input: 541.51 toks/s, output: 48.66 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.70s/it, est. speed input: 405.76 toks/s, output: 49.20 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.40s/it, est. speed input: 186.61 toks/s, output: 50.40 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.45s/it, est. speed input: 235.15 toks/s, output: 50.06 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.37s/it, est. speed input: 253.71 toks/s, output: 49.57 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.36s/it, est. speed input: 303.64 toks/s, output: 49.79 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.28s/it, est. speed input: 261.69 toks/s, output: 49.93 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.46s/it, est. speed input: 470.94 toks/s, output: 49.08 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.23s/it, est. speed input: 419.97 toks/s, output: 49.17 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.14s/it, est. speed input: 532.06 toks/s, output: 48.78 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.35s/it, est. speed input: 505.44 toks/s, output: 48.77 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.08s/it, est. speed input: 570.61 toks/s, output: 48.58 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5h 38min 58s, sys: 24min 28s, total: 6h 3min 27s\n",
      "Wall time: 46min 33s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "book = test_prompt[:]\n",
    "previous_chapter = test_prompt \n",
    "words_per_chapter = 40000//total_chapters\n",
    "print(words_per_chapter)\n",
    "\n",
    "for chapter_i in range(1, total_chapters+1):\n",
    "    is_n_k_words_per_chapter = False\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": start_of_chapter_prompt.format(\n",
    "            chapter_n=chapter_i, \n",
    "            previous_chapter=previous_chapter, \n",
    "            book_summary=summarization(book))},\n",
    "        {\"role\": \"user\", \"content\": book_summary}\n",
    "    ]\n",
    "\n",
    "    part_prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    chapter = llm.generate(part_prompt, chapter_config)[0].outputs[0].text\n",
    "\n",
    "    while not is_n_k_words_per_chapter:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": continue_chapter_prompt.format(\n",
    "                chapter_n=chapter_i, \n",
    "                full_chapter_context=chapter, \n",
    "                book_summary=summarization(book))},\n",
    "            {\"role\": \"user\", \"content\": book_summary}\n",
    "        ]\n",
    "\n",
    "        part_prompt = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "\n",
    "        part_output = llm.generate(part_prompt, chapter_config)[0].outputs[0].text\n",
    "        chapter += part_output\n",
    "\n",
    "        if count_words(chapter) > words_per_chapter:\n",
    "            is_n_k_words_per_chapter = True\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": final_of_chapter_prompt.format(\n",
    "            chapter_n=chapter_i, \n",
    "            full_chapter_context=chapter, \n",
    "            book_summary=summarization(book))},\n",
    "        {\"role\": \"user\", \"content\": book_summary}\n",
    "    ]\n",
    "\n",
    "    part_prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    part_output = llm.generate(part_prompt, chapter_config)[0].outputs[0].text\n",
    "    chapter += part_output\n",
    "    book += chapter\n",
    "    previous_chapter = chapter[:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a07b4226-5874-4167-8bb0-0bb7b0ee2858",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T17:48:57.245986Z",
     "iopub.status.busy": "2024-07-15T17:48:57.245798Z",
     "iopub.status.idle": "2024-07-15T17:48:57.275646Z",
     "shell.execute_reply": "2024-07-15T17:48:57.275104Z",
     "shell.execute_reply.started": "2024-07-15T17:48:57.245970Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48280\n",
      "48244\n"
     ]
    }
   ],
   "source": [
    "print(count_words(book))\n",
    "clean_book = ''.join([char for char in book.encode().decode('unicode_escape') if ord(char) < 128])\n",
    "print(count_words(clean_book))\n",
    "with open(\"llama_3_70b_instruct.txt\", \"w\") as file:\n",
    "    file.write(clean_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbf1377-d0e1-4de9-bb46-e4e9c6e693d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
